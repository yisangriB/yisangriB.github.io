---
layout: post
title: "Latin Hypercube Sampling"
author: "Sang-ri Yi"
categories: journal
tags: [LHS]
image: RandomPhotos/OIMG_20201219_230953_924.jpg
---

### We start with an integration.

$$ S = \int h(x)f(x)dx $$

where $$h(x)$$ is a function. For example, depending on how you set $$h(x)$$, $$S$$ can be mean, variance, or even reliability (failure probability). 

Let us think of two approaches to perform the integration.

* Riemann sum: $$S = \sum^n_{i=1}h(x_i)\Delta V_i$$ 
* Monte carlo integration: $$S = \frac{1}{n}\sum^n_{i=1}h(x_i)$$

If we compare the two method, we get

* Convergence rate of MCS $$ \propto \frac{1}{n^{1/2}} $$ (regardless of dimension)
* Convergence rate of Randomized Riemann sum $$\propto \frac{1}{n^{3/2}} $$ [ref:Theorem 5.5](https://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=9056983&fileOId=9058877)

### What is Latin Hypercube Sampling (LHS)?

Conventional Monte Carlo Sampling (MCS) generates each sample independently using a random number generator ranging 0-1. If you sample 100 realizations, you will find gaps and clusters. LHS is an anti-clustering technique. The system divides each dimension of the domain into 0.01 interval layers to get exactly one sample each layer in every 100 trials.

![image](../assets/img/LHS/Comparision.jpg "Sampling methods")

### Benefits of LHS


LHS gives you unbiased estimates on statistics & it reduces the standard error.


### How is LHS gaining efficiency? How much gain do we expect?

* Published theoretical results for the univariate show that the sampling error of Monte Carlo goes down as O( 1/sqrt(N) ), [2], whereas the sampling error for LHS is O( 1/N ), quadratically faster, for almost all distributions and statistics in common use [3], [5], [7]. In other words, if you need N samples for a desired accuracy using LHS, you'll need N2 samples for the same accuracy using MC.[Ref](https://old.analytica.com/blog/latin-hypercube-vs.-monte-carlo-sampling)


There are two ways of understanding LHS

#### Excluding variability in the main effect

1D case convergence rate (for a continuous function):
* MCS $$ \propto \frac{1}{n^{0.5}} $$ (regardless of dimension)
* Riemann sum $$ \propto \frac{1}{n} $$ (for $$p$$-dimensional case,  $$ \propto \frac{1}{n^{1/d}}$$)
* Stratified sampling (Randomized Riemann sum) $$ \propto \frac{1}{n^{1.5}}$$ [ref](https://dl.acm.org/doi/pdf/10.1145/237170.237265) [ref (Theorem 5.5)?](https://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=9056983&fileOId=9058877)




#### Control variate approach

### Drawbacks of LHS

* Midpoint LHS is biased (Riemann Sum with a fixed step size is always biased): Very rarely, median Latin hypercube can produce incorrect results, specifically when the
model has a periodic function with a period similar to the size of the equiprobable intervals. In such cases, you should use
random Latin hypercube or simple Monte Carlo. If your model has no periodic function of
this kind, you do not need to worry about the reliability of median Latin hypercube sampling. [Ref](https://wiki.analytica.com/images/c/c1/Ugbook_image_markup0418.pdf)


What do we loose by LHS [ref](https://www.linkedin.com/pulse/20140708131747-483951-the-pros-and-cons-of-latin-hypercube-sampling)

* May require more computation time and memory than MC depending on algorithm
References [objection](https://old.analytica.com/blog/latin-hypercube-vs.-monte-carlo-sampling#7)
https://www.maxvalue.com/tip029.htm
* Cannot introduce copula? [objection](https://old.analytica.com/blog/latin-hypercube-vs.-monte-carlo-sampling#7)
https://www.maxvalue.com/tip029.htm
* Precision of results and confidence interval cannot be calculated

* Difficult to add new samples





