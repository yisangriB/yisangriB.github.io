---
layout: post
title: "Principal Component Analysis"
author: "Sang-ri Yi"
categories: journal
tags: [PCA]
image: RandomPhotos/OIMG_20210127_085412_458.jpg
---

### PCA 

### Given condition

* Consider a data matrix $$\boldsymbol{\rm{X}}$$ of size $$n\times p$$ 
* $$n$$ is the number of samples, $$p$$ is the dimension size
* $$\tilde{\boldsymbol{\rm{X}}} = \boldsymbol{\rm{X-\mu_X}}$$ is centered at zero

### Procedure of PCA 

* Perform singular value decomposition on $$\tilde{\boldsymbol{\rm{X}}}$$ or eigenvalue decomposition on $$\tilde{\boldsymbol{\rm{X}}}^\rm{T}\tilde{\boldsymbol{\rm{X}}}/\it{n}$$ 

* Retrieve reduced-dimension representation $$\boldsymbol{\rm{Z'}}$$ and corresponding projection matrix $$\boldsymbol{\rm{P'}}$$ 

* $$\boldsymbol{\rm{\tilde{X} \to Z'}}$$ transformation: $$\boldsymbol{\rm{Z'}}=\boldsymbol{\rm{\tilde{X}P'}} $$

* $$\boldsymbol{\rm{Z' \to \tilde{X}}}$$ transformation: $$\tilde{\boldsymbol{\rm{X}}}=\boldsymbol{\rm{Z'P'}}^\rm{T} $$



### PCA using eigenvalue decomposition

* Data matrix: $$\tilde{\boldsymbol{\rm{X}}}$$ ($$n\times p$$)

* **Covariance matrix: $$ \boldsymbol{\rm{C}} = \tilde{\boldsymbol{\rm{X}}}^\rm{T}\tilde{\boldsymbol{\rm{X}}}/\it{n} = \boldsymbol{\rm{VLV}}^\rm{T} $$ ($$p\times p$$) → perform eigenvalue analysis**

* Eigenvalue analysis: $$ \boldsymbol{\rm{CV}} = \boldsymbol{\rm{VL}} $$, $$ \boldsymbol{\rm{VV}}^\rm{T} = \boldsymbol{\rm{I}} $$

* $$\boldsymbol{\rm{V}}$$: eigenvectors (each column is an eigenvector, $$p\times p$$)

* $$\boldsymbol{\rm{L}}$$: eigenvalues (diagonal matrix of $$\lambda_i $$, $$p\times p$$)

* principal directions/axis/coeff or projection matrix: $$\boldsymbol{\rm{P}} = \boldsymbol{\rm{V}}$$ ($$p\times p$$)

* principal components/score: $$\boldsymbol{\rm{Z}} = \tilde{\boldsymbol{\rm{X}}}\boldsymbol{\rm{V}}$$ ($$n \times p$$)

* principal component variances/latent: $$\boldsymbol{\rm{L}}$$ ($$p \times p$$)

### PCA using singularvalue decomposition(SVD)

* **Data matrix: $$\tilde{\boldsymbol{\rm{X}}} = \boldsymbol{\rm{USV}}^\rm{T} $$ ($$n \times p$$) → perform SVD**

* $$\boldsymbol{\rm{U}} $$ ($$n \times n$$ ) : if $$n>p$$, practically, we use $$n \times p$$

* $$\boldsymbol{\rm{S}} $$ ($$n \times p$$ ) : Singular values. Rectangular diagonal matrix ($$\lambda_i = s_i^2/\it{n} $$)

* $$\boldsymbol{\rm{V}} $$ ($$p \times p$$ ) : if $$p>n$$, practically, we use $$n \times p$$

* Covariance matrix: $$ \boldsymbol{\rm{C}} = \tilde{\boldsymbol{\rm{X}}}^\rm{T}\tilde{\boldsymbol{\rm{X}}}/\it{n} = \boldsymbol{\rm{VS}}^2\boldsymbol{\rm{V}}^\rm{T}/\it{n}$$   ($$p \times p$$)

* principal directions/axis/coeff or projection matrix: $$\boldsymbol{\rm{P}} = \boldsymbol{\rm{V}}$$ ($$p \times p$$)

* principal components/score: $$\boldsymbol{\rm{Z}} = \boldsymbol{\rm{US}}$$ ($$n \times p$$)

* principal component variances/latent: $$\boldsymbol{\rm{S^2}}/\it{n}$$    ($$n \times p$$)

### Dimension reduction

* Reduce the dimension $$p$$ → $$p'$$

* $$\boldsymbol{\rm{P}} (p \times p)$$ → $$\boldsymbol{\rm{P'}} (p \times p')$$

* $$\boldsymbol{\rm{Z}} (n \times p)$$ → $$\boldsymbol{\rm{Z'}} (n \times p')$$

* Approximation $$\boldsymbol{\rm{X}}=\boldsymbol{\rm{ZP}}^\rm{T} + \boldsymbol{\rm{\mu_X}} \simeq \boldsymbol{\rm{Z'P'}}^\rm{T} + \boldsymbol{\rm{\mu_X}} $$ 



Ref: https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca