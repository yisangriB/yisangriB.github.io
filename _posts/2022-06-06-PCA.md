---
layout: post
title: "Principal Component Analysis"
author: "Sang-ri Yi"
categories: journal
tags: [PCA]
image: RandomPhotos/OIMG_20210127_085412_458.jpg
---

### PCA 

### Summary

I needed to deal with a dataset with a huge dimension (2 million) and a relatively fewer sample size (500). It was spatial field data, therefore, highly correlated. I needed to find the most efficient ways to perform PCA.

* Method 1 - gave me a memory error because of the huge correlation matrix.
* Method 2 - ran successfully but took 5 mins.
* Method 3 - took 30 secs. The computational cost was mostly in constructing the gram matrix and the principal direction matrix.

The code was written in c++, and for SVD and eigenvalue decomposition, I used the LAPACK-BLAS library through [Armadillo](http://arma.sourceforge.net/)

### Given condition

* Consider a data matrix $$\boldsymbol{\rm{X}}$$ of size $$n\times p$$ 
* $$n$$ is the number of samples, $$p$$ is the dimension size
* $$\tilde{\boldsymbol{\rm{X}}} = \boldsymbol{\rm{X-\mu_X}}$$ is centered at zero

### Procedure of PCA 

* Perform singular value decomposition on $$\tilde{\boldsymbol{\rm{X}}}$$ or eigenvalue decomposition on $$\tilde{\boldsymbol{\rm{X}}}^\rm{T}\tilde{\boldsymbol{\rm{X}}}/\it{n}$$ 

* Retrieve reduced-dimension representation $$\boldsymbol{\rm{Z'}}$$ and corresponding projection matrix $$\boldsymbol{\rm{P'}}$$ 

* $$\boldsymbol{\rm{\tilde{X} \to Z'}}$$ transformation: $$\boldsymbol{\rm{Z'}}=\boldsymbol{\rm{\tilde{X}P'}} $$

* $$\boldsymbol{\rm{Z' \to \tilde{X}}}$$ transformation: $$\tilde{\boldsymbol{\rm{X}}}=\boldsymbol{\rm{Z'P'}}^\rm{T} $$



### Method 1. PCA using eigenvalue decomposition of corrleation matrix

* Data matrix: $$\tilde{\boldsymbol{\rm{X}}}$$ ($$n\times p$$)

* **Covariance matrix: $$ \boldsymbol{\rm{C}} = \tilde{\boldsymbol{\rm{X}}}^\rm{T}\tilde{\boldsymbol{\rm{X}}}/\it{n} = \boldsymbol{\rm{VLV}}^\rm{T} $$ ($$p\times p$$) → perform eigenvalue analysis**

* Eigenvalue analysis: $$ \boldsymbol{\rm{CV}} = \boldsymbol{\rm{VL}} $$, $$ \boldsymbol{\rm{VV}}^\rm{T} = \boldsymbol{\rm{I}} $$

* $$\boldsymbol{\rm{V}}$$: eigenvectors (each column is an eigenvector, $$p\times p$$)

* $$\boldsymbol{\rm{L}}$$: eigenvalues (diagonal matrix of $$\lambda_i $$, $$p\times p$$)

* principal directions/axis/coeff or projection matrix: $$\boldsymbol{\rm{P}} = \boldsymbol{\rm{V}}$$ ($$p\times p$$)

* principal components/score: $$\boldsymbol{\rm{Z}} = \tilde{\boldsymbol{\rm{X}}}\boldsymbol{\rm{V}}$$ ($$n \times p$$)

* principal component variances/latent: $$\boldsymbol{\rm{L}}$$ ($$p \times p$$)

### Method 2. PCA using singularvalue decomposition(SVD)

* **Data matrix: $$\tilde{\boldsymbol{\rm{X}}} = \boldsymbol{\rm{USV}}^\rm{T} $$ ($$n \times p$$) → perform SVD**

* $$\boldsymbol{\rm{U}} $$ ($$n \times n$$ ) : if $$n>p$$, practically, we use $$n \times p$$

* $$\boldsymbol{\rm{S}} $$ ($$n \times p$$ ) : Singular values. Rectangular diagonal matrix ($$\lambda_i = s_i^2/\it{n} $$)

* $$\boldsymbol{\rm{V}} $$ ($$p \times p$$ ) : if $$p>n$$, practically, we use $$n \times p$$

* Covariance matrix: $$ \boldsymbol{\rm{C}} = \tilde{\boldsymbol{\rm{X}}}^\rm{T}\tilde{\boldsymbol{\rm{X}}}/\it{n} = \boldsymbol{\rm{VS}}^2\boldsymbol{\rm{V}}^\rm{T}/\it{n}$$   ($$p \times p$$)

* principal directions/axis/coeff or projection matrix: $$\boldsymbol{\rm{P}} = \boldsymbol{\rm{V}}$$ ($$p \times p$$)

* principal components/score: $$\boldsymbol{\rm{Z}} = \boldsymbol{\rm{US}}$$ ($$n \times p$$)

* principal component variances/latent: $$\boldsymbol{\rm{S^2}}/\it{n}$$    ($$n \times p$$)


### Method 3. PCA using eigenvalue decomposition of gram (gramian) matrix

* Data matrix: $$\tilde{\boldsymbol{\rm{X}}}$$ ($$n\times p$$)

* **Gram matrix: $$ \boldsymbol{\rm{G}} = \tilde{\boldsymbol{\rm{X}}}\tilde{\boldsymbol{\rm{X}}}^\rm{T} = \boldsymbol{\rm{UL_GU}}^\rm{T} $$ ($$n\times n$$) → perform eigenvalue analysis**

* Eigenvalue analysis: $$ \boldsymbol{\rm{GU}} = \boldsymbol{\rm{UL_G}} $$, $$ \boldsymbol{\rm{UU}}^\rm{T} = \boldsymbol{\rm{I}} $$

* $$\boldsymbol{\rm{U}}$$: eigenvectors (each column is an eigenvector, $$n\times n$$)

* $$\boldsymbol{\rm{L_G}}$$: eigenvalues (diagonal matrix of $$\lambda_{G,i} =n\lambda_{i}=s_i^2 $$, $$n\times n$$)

* principal directions/axis/coeff or projection matrix: $$\boldsymbol{\rm{P}} = \boldsymbol{\rm{V}} = \boldsymbol{\rm{S}}^{-1}\boldsymbol{\rm{U}}^\rm{T}\boldsymbol{\rm{\tilde{X}}}  = \boldsymbol{\rm{L_G}}^{-\frac{1}{2}}\boldsymbol{\rm{U}}^\rm{T}\boldsymbol{\rm{\tilde{X}}} $$ ($$p\times p$$)

* principal components/score: $$\boldsymbol{\rm{Z}} = \boldsymbol{\rm{US}}$$ ($$n \times p$$)

* principal component variances/latent: $$\boldsymbol{\rm{L_G}}/n^2$$ ($$p \times p$$)


### Dimension reduction

* Reduce the dimension $$p$$ → $$p'$$

* $$\boldsymbol{\rm{P}} (p \times p)$$ → $$\boldsymbol{\rm{P'}} (p \times p')$$

* $$\boldsymbol{\rm{Z}} (n \times p)$$ → $$\boldsymbol{\rm{Z'}} (n \times p')$$

* Approximation $$\boldsymbol{\rm{X}}=\boldsymbol{\rm{ZP}}^\rm{T} + \boldsymbol{\rm{\mu_X}} \simeq \boldsymbol{\rm{Z'P'}}^\rm{T} + \boldsymbol{\rm{\mu_X}} $$ 



Ref: https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca